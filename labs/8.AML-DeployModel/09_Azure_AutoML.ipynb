{"cells":[{"cell_type":"markdown","source":["# Azure AutoML \n\n## Install Azure Auto ML sdk \n\nAdd ```azureml-sdk[automl_databricks]``` library to your Databricks cluster"],"metadata":{}},{"cell_type":"code","source":["# Set up notebook parameters\ndbutils.widgets.text(\"STORAGE_ACCOUNT\", \"azureailabs\")\ndbutils.widgets.text(\"CONTAINER\", \"ingest\")\ndbutils.widgets.text(\"ACCOUNT_KEY\", \"\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Load data from Azure Blob\nSTORAGE_ACCOUNT = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\nCONTAINER = dbutils.widgets.get(\"CONTAINER\").strip()\nACCOUNT_KEY = dbutils.widgets.get(\"ACCOUNT_KEY\").strip()\n\nif ACCOUNT_KEY != \"\":\n  # Set up account access key\n  conf_key = \"fs.azure.account.key.{storage_acct}.blob.core.windows.net\".format(storage_acct=STORAGE_ACCOUNT)\n  spark.conf.set(conf_key, ACCOUNT_KEY)\n\nsource_str = \"wasbs://{container}@{storage_acct}.blob.core.windows.net/\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\n  \n# Read the data from the default datasets repository in Databricks\ndf = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(source_str)\ndisplay(df)\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nschema = StructType([\n  StructField(\"age\", DoubleType()),\n  StructField(\"annualincome\", DoubleType()),\n  StructField(\"calldroprate\", DoubleType()),\n  StructField(\"callfailurerate\", DoubleType()),\n  StructField(\"callingnum\", StringType()),\n  StructField(\"customerid\", StringType()),\n  StructField(\"customersuspended\",  StringType()),\n  StructField(\"education\",  StringType()),\n  StructField(\"gender\", StringType()),\n  StructField(\"homeowner\", StringType()),\n  StructField(\"maritalstatus\", StringType()),\n  StructField(\"monthlybilledamount\", DoubleType()),\n  StructField(\"noadditionallines\", StringType()),\n  StructField(\"numberofcomplaints\", DoubleType()),\n  StructField(\"numberofmonthunpaid\", DoubleType()),\n  StructField(\"numdayscontractequipmentplanexpiring\", DoubleType()),\n  StructField(\"occupation\", StringType()),\n  StructField(\"penaltytoswitch\", DoubleType()),\n  StructField(\"state\", StringType()),\n  StructField(\"totalminsusedinlastmonth\", DoubleType()),\n  StructField(\"unpaidbalance\", DoubleType()),\n  StructField(\"usesinternetservice\", StringType()),\n  StructField(\"usesvoiceservice\", StringType()),\n  StructField(\"percentagecalloutsidenetwork\", DoubleType()),\n  StructField(\"totalcallduration\", DoubleType()),\n  StructField(\"avgcallduration\", DoubleType()),\n  StructField(\"churn\", DoubleType()),\n  StructField(\"year\", DoubleType()),\n  StructField(\"month\", DoubleType())\n])\n\ndf = (spark.read\n     .option(\"header\", True)\n     .schema(schema)\n     .csv(source_str))\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["clean_df = (df.drop(\"customerid\", \"callingnum\", \"year\", \"month\")\n    .dropDuplicates()\n    .filter(~ ((df.age<14) & (df.annualincome>10000))))\n  \ndisplay(clean_df.groupBy(\"churn\").count())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\nfrom pyspark.sql.types import *\nfrom pyspark.ml import Pipeline\n\n# Create a list of string indexers - one for each string column\nstringCols = [field.name for field in clean_df.schema if field.dataType == StringType()]\nstringIndexers = [StringIndexer().setInputCol(name).setOutputCol(name+\"_idx\") for name in stringCols]\n\n# Create a pipeline\nstages = stringIndexers\npipeline = Pipeline(stages=stages)\n\n# Check the Pipeline operation\nindexed=pipeline.fit(clean_df).transform(clean_df)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Split the dataset randomly into 85% for training and 15% for testing.\n\ntrain, test = indexed.select([column for column in indexed.columns if column not in stringCols]).randomSplit([0.85, 0.15], 0)\nprint(\"We have {} training examples and {} test examples.\".format(train.count(), test.count()))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["pdTrain=train.toPandas()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["label=['churn']\nx_train = train.select([column for column in pdTrain.columns if column not in label]).toPandas()\ny_train = train.select([column for column in pdTrain.columns if column in label]).toPandas()\ny_train = y_train.pop('churn').values"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["print(x_train.shape)\nprint(y_train.shape)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["subscription_id = \"USE YOUR SUB ID\"\nresource_group = \"USE YOUR RG NAEM\"\nworkspace_name = \"USE YOUR WS\"\nworkspace_region = \"USE YOUR REGION\"\n\n# import the Workspace class and check the azureml SDK version\n# exist_ok checks if workspace exists or not.\n\nfrom azureml.core import Workspace\n\nws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      exist_ok=True)\n\nws.get_details()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["import azureml.core\nfrom azureml.core.experiment import Experiment\nimport pandas as pd\n\nexperiment_name =  'customer-churn-automl-exp' # choose a name for experiment\nproject_folder = './customer-churn-automl' # project folder\n\nexperiment=Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data=output, index=['']).T"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from azureml.train.automl import AutoMLConfig\n\n##Local compute \nAutoml_config = AutoMLConfig(task = 'classification',\n                             primary_metric = 'AUC_weighted',\n                             iteration_timeout_minutes = 12000,\n                             iterations = 10,\n                             n_cross_validations = 3,\n                             preprocess = False,\n                             experiment_exit_score = 0.9000,\n                             blacklist_models = ['LightGBM','kNN','LinearSVM'],\n                             X = x_train,\n                             y = y_train,\n                             path=project_folder)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from azureml.core.experiment import Experiment\nexperiment=Experiment(ws, experiment_name)\nlocal_run = experiment.submit(Automl_config, show_output=True)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Install library to cluster\n\n```\nazureml-widgets\nazureml-explain-model\n```"],"metadata":{}},{"cell_type":"code","source":["from azureml.widgets import RunDetails\nRunDetails(local_run).show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["best_run, fitted_model = local_run.get_output()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["best_run.get_details()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"03_04_Azure_AutoML","notebookId":1885667120396975},"nbformat":4,"nbformat_minor":0}
