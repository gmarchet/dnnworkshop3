{"cells":[{"cell_type":"markdown","source":["# Spark ML model operationalization with Azure Machine Learning service.\n\nIn this lab, you will operationalize the Spark ML model developed in Lab 03 as a REST web service running in Azure Container Instance.\nAzure Machine Learning service helps you orchestrate machine learning workflows using the architecture depicted on the below diagram.\n\n![AML workflow](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/amlarch.png)"],"metadata":{}},{"cell_type":"markdown","source":["## Install Azure ML SDK \n\nBefore you can use Azure ML service features from Azure Databricks, you need to install Azure ML SDK as Azure Databricks Library. Follow this instructions:\n\nhttps://docs.databricks.com/user-guide/libraries.html \n\nand add `azureml-sdk[databricks]` as your PyPi package. You can select the option to attach the library to all clusters or just one cluster."],"metadata":{}},{"cell_type":"markdown","source":["### Check SDK Version"],"metadata":{}},{"cell_type":"code","source":["import azureml.core\n\nprint(\"SDK version:\", azureml.core.VERSION)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Connect to Azure ML workspace\n\nFollow the instructor to create Azure ML Workspace using Azure Portal. \n\nAfter the workspace has been provisioned, execute the below cells to connect to the workspace and save connection information on a driver node."],"metadata":{}},{"cell_type":"code","source":["# Set connection parameters\nsubscription_id = \"<your subscription>\"\nresource_group = \"<your resource gropup>\"\nworkspace_name = \"<your workspace name>\"\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from azureml.core import Workspace# Connect to the workspace\n\nfrom azureml.core import Workspace\n\nws = Workspace(workspace_name = workspace_name,\n               subscription_id = subscription_id,\n               resource_group = resource_group)\n\n# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\nws.write_config()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Review the AML config file."],"metadata":{}},{"cell_type":"code","source":["%sh\ncat /databricks/driver/aml_config/config.json"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Register your model \n\nOne of the key features of Azure Machine Learning service is **Model Registry**. You can use model registry to manage versions of models including arbitrary meta data about the models.\n\nBefore you call the AML model register API you need to copy the model to the driver node, as the model register API searches for model files in the local (driver) file system."],"metadata":{}},{"cell_type":"code","source":["import os\n\nmodel_dbfs_path = '/models/churn_classifier'\nmodel_name = 'ChurnClassifierML'\nmodel_local = 'file:' + os.getcwd() + '/' + model_name\n\ndbutils.fs.cp(model_dbfs_path, model_local, True)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sh\n\nls -la ."],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["You can now register the model."],"metadata":{}},{"cell_type":"code","source":["from azureml.core.model import Model\n\nmymodel = Model.register(model_path=model_name, \n                         model_name=model_name,\n                         description='Spark ML classifier model for customer churn prediction',\n                         workspace=ws\n                        )\n\nprint(mymodel.name, mymodel.description, mymodel.version)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["The model has been registered with the model registry. The next step is to deploy the model to Azure Container Instance.\n\n## Deploy the model to ACI\n\nTo build the correct environment for ACI deployment, you need to provide the following:\n* A scoring script that invokes the model\n* An environment file to show what packages need to be installed\n* A configuration file to build the ACI\n* The serialized model \n\n### Create scoring script\n\nCreate the scoring script, called score.py, used by the web service call to invoke the model.\n\nYou must include two required functions in the scoring script:\n* The `init()` function, which loads the model into a global object. This function is run only once when the Docker container is started. \n\n* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats can be used."],"metadata":{}},{"cell_type":"code","source":["%%writefile score.py\n\nimport json\nimport pyspark\nfrom azureml.core.model import Model\nfrom pyspark.ml import PipelineModel\n\n\ndef init():\n    try:\n        # One-time initialization of PySpark and predictive model\n        \n        global trainedModel\n        global spark\n        \n        spark = pyspark.sql.SparkSession.builder.appName(\"Churn prediction\").getOrCreate()\n        model_name = \"<<model_name>>\" \n        model_path = Model.get_model_path(model_name)\n        trainedModel = PipelineModel.load(model_path)\n    except Exception as e:\n        trainedModel = e\n\ndef run(input_json):\n    if isinstance(trainedModel, Exception):\n        return json.dumps({\"trainedModel\":str(trainedModel)})\n      \n    try:\n        sc = spark.sparkContext\n        input_list = json.loads(input_json)\n        input_rdd = sc.parallelize(input_list)\n        input_df = spark.read.json(input_rdd)\n    \n        # Compute prediction\n        prediction = trainedModel.transform(input_df)\n        #result = prediction.first().prediction\n        predictions = prediction.collect()\n\n        #Get each scored result\n        preds = [str(x['prediction']) for x in predictions]\n        # result = \",\".join(preds)\n        result = preds\n    except Exception as e:\n        result = str(e)\n    return json.dumps({\"result\":result})        "],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sh\ncat score.py"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Substitue the actual *model name* in the script file."],"metadata":{}},{"cell_type":"code","source":["script_file_name = 'score.py'\n\nwith open(script_file_name, 'r') as cefr:\n    content = cefr.read()\n    \nwith open(script_file_name, 'w') as cefw:\n    cefw.write(content.replace('<<model_name>>', mymodel.name))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sh\ncat score.py"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Create a docker image encapsulating the model\n\nThe docker image that encapsulates our model is be based on a standard AML image that contains the PySpark runtime and a web service wrapper. It must also include the scoring script, any dependencies required by the scoring script, and `azureml-defaults`."],"metadata":{}},{"cell_type":"code","source":["from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n    \n# Review Conda dependencies file\nwith open(\"myenv.yml\",\"r\") as f:\n    print(f.read())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from azureml.core.image import ContainerImage, Image\n\nruntime = \"spark-py\"\nscoring_script = \"score.py\"\n\n# Configure the image\nimage_config = ContainerImage.image_configuration(execution_script=scoring_script, \n                                                  runtime=runtime, \n                                                  conda_file=\"myenv.yml\",\n                                                  description=\"Churn prediction web service\",\n                                                  tags={\"Classifier\": \"GBT\"})\n\n# Create image\nimage = Image.create(name = \"churn-classifier\",\n                     # this is the model object \n                     models = [mymodel],\n                     image_config = image_config, \n                     workspace = ws)\n\nimage.wait_for_creation(show_output = True)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Define ACI configuration\n\nCreate a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. The default is 1 core and 1 gigabyte of RAM.  In this lab we will use the defaults but you should always go through the proper performance plannig exercise to find the right configuration."],"metadata":{}},{"cell_type":"code","source":["from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={\"Model\": \"GBT\"}, \n                                               description='Predict customer churn')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Deploy in ACI\n\nDeploy the image as a web service in Azure Container Instance."],"metadata":{}},{"cell_type":"code","source":["from azureml.core.webservice import Webservice\n\naci_service_name = 'churn-classifier'\nprint(aci_service_name)\naci_service = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                           image = image,\n                                           name = aci_service_name,\n                                           workspace = ws)\naci_service.wait_for_deployment(True)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["print(aci_service.get_logs())"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### Test the prediction web service\n\nThe web service encapsulating the model has been started and is accessible using the following URL."],"metadata":{}},{"cell_type":"code","source":["print(aci_service.scoring_uri)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["To test the service we will use 5 rows from the testing dataset that was saved as a parquet file in the previous lab. As you recall, the `run` function in the scoring script assumes that the data is formatted as JSON."],"metadata":{}},{"cell_type":"code","source":["import json\n\n# Read 5 rows fro the test dataset\ntest_data = spark.read.parquet(\"/datasets/churn_test_data\").limit(5)\n\n# Convert it to JSON\ntest_json = json.dumps(test_data.toJSON().collect())\nprint(test_json)\n"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Call the web service."],"metadata":{}},{"cell_type":"code","source":["aci_service.run(input_data=test_json)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["print(aci_service.get_logs())"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["### Clean up"],"metadata":{}},{"cell_type":"code","source":["#Delete service\n\naci_service.delete()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":38}],"metadata":{"name":"07- MLlib Classification Deployment","notebookId":126212991376695},"nbformat":4,"nbformat_minor":0}
