{"cells":[{"cell_type":"markdown","source":["This notebook steps throgh how to implement an image recognition model using a convolutional network.  You will start with a basic CNN architecture to classify fashion from the Fashion MNIST data set found here:  https://github.com/zalandoresearch/fashion-mnist. The data set consists of 10,000 clothing items classified into 10 categories.\n    \nThere are optional steps in the notebook to demonstrate use of more advanced network features. The model is built using Keras and Tensorflow and assumes you are runnig on a Databricks cluter with ML and DL frameworks and libraries pre-installed. \n\nThis notebook is adapted from work published here:(1) https://github.com/amynic/azureml-sdk-fashion and (2) https://github.com/Microsoft/CNTK/tree/v2.0/Tutorials."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nimport os\nimport time\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]= \"2\"\nprint(\"tensorflow Version is: \" + str(tf.__version__))\n\nimport numpy as np\nos.environ['KERAS_BACKEND'] = 'tensorflow'\nfrom keras import backend as K\nprint(os.environ['KERAS_BACKEND'])"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">tensorflow Version is: 1.12.0\ntensorflow\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Import the required Keras functions that will be used to create the CNN\nfrom keras.datasets import fashion_mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import utils, losses, optimizers\nimport matplotlib.pyplot as plt"],"metadata":{"collapsed":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["The fasion MNIST data set is included with the Keras library so we need to pull it into a training and test datasets.  The dataset consists of 60,000 training images and 10,000 test images.  Each image is associated with a label from 10 classifications: \nLabel    Description\n0        T-shirt/top\n1        Trouser\n2        Pullover\n3        Dress\n4        Coat\n5        Sandal\n6        Shirt\n7        Sneaker\n8        Bag\n9        Ankle boot"],"metadata":{}},{"cell_type":"code","source":["#data for train and testing\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\nprint(x_train.shape, 'train set')\nprint(x_test.shape, 'test set')\n\n# Define the text labels\nfashion_mnist_labels = [\"Top\",          # index 0\n                        \"Trouser\",      # index 1\n                        \"Jumper\",       # index 2 \n                        \"Dress\",        # index 3 \n                        \"Coat\",         # index 4\n                        \"Sandal\",       # index 5\n                        \"Shirt\",        # index 6 \n                        \"Trainer\",      # index 7 \n                        \"Bag\",          # index 8 \n                        \"Ankle boot\"]   # index 9\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(60000, 28, 28) train set\n(10000, 28, 28) test set\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Visualize an image. To visualize a different image, set the index to any value between 0 and 59999\nimg_index=10\n\n# Display an image from the data set\nlabel_index = y_train[img_index]\nplt.imshow(x_train[img_index])\nprint('Label Index: ' + str(label_index) + \" Fashion Labels: \" + (fashion_mnist_labels[label_index]))\ndisplay()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuwrWd9H/bvb9+lcxNCkiVZAoSQXa6WjUkIQ4A6xsng1OC0M7imsc1kGg+OM8XuNB0xaU2mddQLpkyBTjLUMzBJXLupbWxsFMtgZJtgKJfYQdg1FGSQEEd3zjk6l319+sfeh24OR5dnWXutc/bz+cys2dprra/e3373u/b5rne9613VWgsAAOOYm/UAAABMlwIIADAYBRAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMZmHWA3BxqqpKcm2SE7OeBYCJHUpyb2utzXoQpksBZFLXJrln1kMA8Jd2XZKvznoIpksBZFInkuTleU0WsjjrWYYwf9nhiXJ//t/f2J35j170J92ZD/3mS7oz3/62T3Rn+Mt56I1/pTtz/eu/1J3589/v3+6uv9X2ME0bWc9H88HEKzlDUgAHV1U/leS/SnJNks8leXNr7Q+fbH4hi1koBXAa5mtpotzcJSvdmeWD/b/T+eX+5dh2pm9+qf/3tHigf9ubX7E9XPC86Ds0bwIZWFW9Psk7kvx8ku9O8odJbquqZ8x0MABgTymAY/vZJL/YWvvfW2t/1lp7c5K7k7xpxnMBAHtIARxUVS0leXGS28+56fYkLzvP/Zer6vDZS7bfOQYAXIQUwHFdkWQ+yX3nXH9fkqvPc/9bkhzbdfEOYAC4SCmAnHsYcJ3nuiS5NcmRXZfr9nguAGCPeBfwuB5Msplv3dt3Vb51r2Baa6tJVs9+v30eaADgYmQP4KBaa2tJPp3k1efc9OokH5v+RADAtNgDOLa3J/kXVfWpJH+U5O8neUaSfzbTqQCAPaUADqy19itV9fQk/222TwR9Z5LXtNa+PNvJAIC9VD7/mUnsnArm2KvyWmfvn8AXf+nm7szP3PzhiZa1UuvdmY8f7/8Yr39w1e91Z/7vMzd0Z5LkQw89tzvz6bv6z2++daJ/2164bK0786YX/UF3JkmOzJ/qzty0fLQ78+ETz+/OPGPpoe7M7z78vO5Mkhx701Xdma1///9MtKz9ZKOt5478RpIcaa0dn/U8TJdjAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwmGqtzXoGLkJVdTjJsVfltVmoxVmPM1Mn/+O/2p256r/4UnfmL75+eXcmSa46+Gh3Zq76/y5cvnyqO/M9h7/SnUmSaxcf6c589Ph3dGc++LkXdGf+9gv+fXfm6YsnuzNJ8sVTV3Rn/uyhq7sz33n5/d2Zu473b6/XH/p6dyZJjp483J1Z/oG/mGhZ+8lGW88d+Y0kOdJaOz7reZguewABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAuzHgAudl/9G607c989396dWVpe784kyZmNxe7MykL/sv7fr1/RnTmzOdmfoLnqX+dLc5vdmb9y013dmYfXDnRnjp453J1JkqMn+3Pfc9Xd3ZkHzhzszsxP8Du6875rujNJcsXBk92Z1R98SXdm+bc/2Z2BC5U9gAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYzGSfxA58w4Gr+z+I/tSJ5f4FTRBJkjMb/Q/zxfnN7syBpbXuzKPrk/1QD5060J1ZXtjozsxV686sb/U/r77mwPHuTJJcvnKqO/PAmYPdmftOHerObLXqzszPbXVnJl3W0b/e/7i44be7I3DBsgcQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBKIAAAIPp/zRs2M/m5rsjVxw82Z35yvGV7sypCTJJcuny+kS5XsvzG92ZlfkJZ7u0P7IywXwnN5a6M5ekdWcW5ra6M0myMr/anVms/mVdutD/e3p4dYJf0oQ2W3Vn5m98dA8mgYuHPYAAAINRAAEABqMADqqq3lpV7ZzL0VnPBQDsPccAju1zSb5/1/ebsxoEAJgeBXBsG601e/0AYDBeAh7bTVV1b1XdVVW/XFXPfqw7VtVyVR0+e0lyaIpzAgBPIQVwXJ9I8mNJ/maS/zzJ1Uk+VlVPf4z735Lk2K7LPdMYEgB46imAg2qt3dZa+9XW2mdbax9K8oM7N/34Y0RuTXJk1+W6KYwJAOwBxwCSJGmtnayqzya56TFuX03yjbPOVvWfeBUAuDDYA0iS7WP8kjw3yddmPQsAsLcUwEFV1duq6pVVdUNV/dUk/1eSw0neN+PRAIA95iXgcV2X5P9IckWSB5J8PMlLW2tfnulUAMCeUwAH1Vr7kVnPcCGae+F3dGfm5052ZxZW1rsz68eXuzNJ8sixA92ZpYWN7syNR451Z85sLnZnkuTg4uoT3+kcc9W6Mwtz/edGn2Q5pzaWujNJsjLfvx1NMt9G63+xaKv1Hyd84vRKd2ZSz/22/lOg9j/S4cLlJWAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADGZh1gPAheT0dQe7M2fW1rszbWuC517VH0mSubtXujMPzG11Z75+8pLuTE34Mx259HR3Zm2j/8/d5lb/gJMsZ3F+szuTJI8s96/zzQm2vdNri92Z4/f1P5bmLt3oziTJpQdXuzN/8fXLuzPXXL/cndm4+57uDEyDPYAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwmIVZDwAXklNX9j8kHrjvSHfm0sNnujNvvvnD3Zkkecdv/e3uzNbRS7oz7dv6f6al5fXuTJI8ema5O7O23v+7ba07kq3N/ufVazXfv6Aky4sb3ZnVCdbD8QcOdmd+4Lvv7M5sbE22Hn7/S8/pziwePN2defTma7szK3ff052BabAHEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACD6f9UcNjHTl9Z3ZnlA2vdmVtf9OvdmZcs39+dSZJ/ffOLuzNH/6j/Q++vet6x7swDxw92Z5Jkbav/uevc3FZ3Zn19vjuzuLTRnVmY758tSQ4tr3ZnnnXk4e7MJ756uDvzwJn+3+3/8Mz3d2eS5PKlk92Zj91/Q3fmge/q/yfz+g90R2Aq7AEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGCqtTbrGbgIVdXhJMdelddmoRZnPc5MzT/vO7ozj/4v692Zg/9wsudrn//JK7szdc2Z7syhg6e7M8cfvaQ7kySLi5sT5XrNzW11Z6r6l7OxMdnv9tClq92Z5z79aHdmbWuhO3PiP1nqzvzZW57ZnUmSlWtOdmee+WNf6s5snTrVnbmQbbT13JHfSJIjrbXjs56H6bIHEABgMAogAMBgFMB9qKpeUVUfqKp7q6pV1evOub2q6q07t5+uqjuq6vmzmhcAmC4FcH86kORPkvz0Y9z+j5L87M7tL0lyNMnvVtWh6YwHAMxS/5G9XPBaa7cluS1J6pwj0mv7ijcn+fnW2q/tXPfjSe5L8qNJ/vlUhwUAps4ewPHckOTqJLefvaK1tprk95O87LFCVbVcVYfPXpLYWwgAFykFcDxX73y975zr79t12/nckuTYrss9T/1oAMA0KIDjOvcEkHWe63a7NcmRXZfr9mguAGCPOQZwPGfPAnt1kq/tuv6qfOtewW/YeZn4G2edPffYQgDg4mEP4HjuynYJfPXZK6pqKckrk3xsVkMBANNjD+A+VFUHkzxn11U3VNXNSR5urX2lqt6R5C1V9YUkX0jyliSnkvzS9KcFAKZNAdyfvjfJR3Z9//adr+9L8hNJ/qcklyT535I8LcknkvxAa+3EFGcEAGakWnu84/7h/HZOBXPsVXltFmpx1uPwOB76e3+tO/PsN36+O3Pn0Wu6M+trkz0HnV/Y7M5MctjqwgTLmavp/U09fXqpO3Pz9f1v4F+a618PD7zs690ZpmujreeO/EaSHGmtHZ/1PEyXYwABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAuzHgAuKFX9kfn5/uVMkGmrq/3LSXLFZ453Z+5//aHuTGsTrLu5re5MkiwubnZnNjb61/nWVv/PNMnT6oUJ18Mk6/yhMwe6My+/8ovdmQey2J2ZVC1M55+ytrExleXANNgDCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBTOcTtOFi0Vp/ZJIPiN/c7M9MaP7YyaksZ319vjuzvLw+0bI2NvqXNT+/1Z2ZYHPIXPWHtlr1LyjJ8kr/+nvk1CXdmUc3lrszSf/6nlSb5PE0yS8X9hF7AAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwmIVZDwAjqoXF7kxbX5toWW25f1mrm1vdma31/ueTC5f2LydJTq/Pd2dWlja7M+ub/cuZq9ad2dia7Ln4wZXV7szptf7t4fav/AfdmWvzp92ZidUE66/1bw+wn9gDCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBLMx6AGBvnXrWZd2Z1fXj3ZmF5Y3uzKQOXrranVnbmM6fu61W3ZmlhcnW3ep6/880V607M8nPNP8dN3ZnNj//xe5MktRc/3xta6JFwb5hDyAAwGAUQACAwSiA+1BVvaKqPlBV91ZVq6rXnXP7e3eu3335+KzmBQCmSwHcnw4k+ZMkP/049/k3Sa7ZdXnNFOYCAC4A3gSyD7XWbktyW5JUPebB0auttaNTGwoAuGDYAziuV1XV/VX1+ap6T1Vd9Xh3rqrlqjp89pLk0JTmBACeYgrgmG5L8oYk35fkv0zykiS/V1XLj5O5JcmxXZd79npIAGBveAl4QK21X9n17Z1V9akkX07yg0l+7TFityZ5+67vD0UJBICLkgJIWmtfq6ovJ7npce6zmuQbZ999nGMLAYALnJeASVU9Pcn1Sb4261kAgL1nD+A+VFUHkzxn11U3VNXNSR7eubw1ya9mu/A9K8k/TfJgkl+f6qAAwEwogPvT9yb5yK7vzx67974kb0rywiQ/luSybJfAjyR5fWvtxDSHBABmQwHch1prdyR5vIP0/uaURuGxTPGT6I/+tf6H+cJGf2ZpabM7Mz832Xo4s7bYnTmwstadOT3Bcja3+o+sObiy+sR3Oo/jp1e6MwsTrPNJ5lv79iPdmfnPd0d2gvP9mY2NCRcG+4NjAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwC7MeAEbUNjentqz1G870hzb6nxseuGS1O7OyuNGdSZIza4vdmaWF/mWtbcx3Zza3pve8+sDyWnfmxOnl7szK0np35qHnrnRnrvpId2TbVpswCOOyBxAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1mY9QBw0Zub789sbXZHanGpfzlJrrrieHfm1Gr/slqr7kx/YnIHF9e6M6fXFrszG5v9z6vnq3VnkuTMBMuam+tf1up6/z8Vx2/a6s5c1Z3Y1jb7H08wOnsAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYjAIIADCY/k/4Br5JzVV3pm31L2f+isv7Q0keeORQd+bqy493Zx45eUl35soDJ7szSXL/ev/PND83wUqfwMJ8/3Lmqk20rMUJltXaZndmaaE/c/CGY92ZiW31z5fqf9ymTfZ7gguRPYAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGMzCrAeAi15N53nU2nOumSh36MDp7swkH3m/srTenTmwuDrBkpLWqjtzcIJlXbq00p05ubrUndma4OdJkiPLZ7ozD2wc6M6sbcz3Z9b7/3mp5eXuTJK01f7fbc33/0xtY6M7AxcqewABAAajAAIADEYB3Ieq6paq+mRVnaiq+6vq/VX1nefcZ7mq3llVD1bVyar6zaq6blYzAwDTowDuT69M8u4kL03y6mwf63l7Ve0++OcdSX44yY8keXmSg0l+q6r6D4wBAC4q3gSyD7XW/tbu76vqjUnuT/LiJH9QVUeS/L0kf7e19qGd+/xnSe5O8v1Jfme6EwMA02QP4BiO7Hx9eOfri5MsJrn97B1aa/cmuTPJy873P9h5yfjw2UuSQ3s4LwCwhxTAfa6qKsnbk3y0tXbnztVXJ1lrrT1yzt3v27ntfG5JcmzX5Z49GBcAmAIFcP97V5IXJflPn8R9K499Crhbs70n8ezFG0YA4CLlGMB9rKremeSHkryitbZ7j93RJEtV9bRz9gJeleRj5/t/tdZWk3zjbKvbOxYBgIuRPYD7UG17V5K/k+T7Wmt3nXOXTydZz/Y7hM9mrknygjxGAQQA9g97APendyf50SSvTXKiqs4e13estXa6tXasqn4xyS9U1UPZfnPI25J8NsmHZjIxADA1CuD+9Kadr3ecc/0bk7x3579/JslGkv8zySVJPpzkJ1prm1OYDwCYIQVwH2rtiT9ZvrV2Jsk/3LlwEXjo+SsT5b7t0P3dma8eO/LEdzrHtYePd2dOri93Z5JkfqH/ecrK/Hp35rKV092Zk6tL3ZnT64vdmSR5xqFz38j/xE6u9883yc90yfJad2b+yiu6M0mycc9X+0PlCCjG5hEAADAYBRAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1mY9QDAk7P6tJood3jpTHfmL9Yv78484+Aj3ZkvHLuyO5MkCwtb3Zmt1v98d6H6l7O8uNGdOXbyku5Mktx44IHuzNdOHe7OrG70/1OxML/ZnVl/xhXdmSSpe746UQ5GZg8gAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAbT/wnfwDebq6ks5tQzNybKPbq+3J2pCX6ka1e+3p352D3P6l9QkpWl9YlyvZ5x4OHuzN3Hj3Rn1tfnuzNJcsPyA92Zzy1f0505ubbUnZmr1p1ZO9K/nCTp38IztcctXKjsAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBZmPQDwJG1NFnt0bbk7c+nKanfm2MYl3Zn19fnuTJIsL250Z65ZOdadeeGld3dn/nDrxu7M4uJmd2ZSC3P9G9L6Zv++gpWF/t9Rte7IxGq+f9ub4niw5+wBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFmY9APDkzK1N9nxtfas/t7K40Z357CPXdmfaBLMlyZm1xe7MwfnV/uW0pe7MsWOXdmeWVta7M0ny5dUrujMLtdWd2Zrw99Rr4XT/djeptrk5tWXBhcgeQACAwSiAAACDUQD3oaq6pao+WVUnqur+qnp/VX3nOfe5o6raOZdfntXMAMD0KID70yuTvDvJS5O8OtvHet5eVQfOud97klyz6/KT0xwSAJgNbwLZh1prf2v391X1xiT3J3lxkj/YddOp1trRac4GAMyePYBjOLLz9eFzrn9DVT1YVZ+rqrdV1aHH+h9U1XJVHT57SfKY9wUALmz2AO5zVVVJ3p7ko621O3fd9K+S3JXkaJIXJLk1yXdl+yXj87klyc/t4agAwJQogPvfu5K8KMnLd1/ZWnvPrm/vrKovJPlUVX1Pa+0z5/n/3JrtInnWoST3PNXDAgB7TwHcx6rqnUl+KMkrWmtPVNY+k2Q9yU07//1NWmurSb5xJt3tHYsAwMVIAdyHdl72fWeSH07yqtbaXU8i9vwki0m+tpezAQCzpwDuT+9O8qNJXpvkRFVdvXP9sdba6aq6MckbknwwyYNJnpfkF5L8uyT/dgbzAgBT5F3A+9Obsv3O3zuyvUfv7OX1O7evJfkbSX4nyZ8n+V+T3J7k+1trPiATAPY5ewD3odba4x6g11q7O9sniwYABqQAwkXishvPPY3jk3P9oa93Z05tLHVnnn3wwf7MoYe6M0lyeOF0d+Z7D3ypO3PTYv98H3zmC7sz333Z3d2ZJPm5K/+0O/PTa/2n8Lzi4MnuzFxadyarXoCAafESMADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGszDrAeCitzmdD7B/9I+fPlHuk0+/rDuz/ED/n4a7Vm/ozqw82LozSVITrPLbrnlpd+bM1f0LuvyP+59Xf3n5xu5MkvzL61/ZnakJljN/aoLUC090R5795fv7l5NkY5LQlB63cKGyBxAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMH4LGD+Ujaynkz2ca77RrX+FdBa/6eXbp05051Jkq3T/Z95unlmgj8Na/2RzbXpfRbw5mr/59lOtO7W+p9Xb9Ykn9CbbJ3pX38TLWmCdZdT/dvrxtYEG1GSjbbenZnW4/ZCtpH+9cb+UW2CBwFU1bcnuWfWcwDwl3Zda+2rsx6C6VIAmUhVVZJrk5w4z82Hsl0Or3uM20dhPWyzHrZZD9ush20Xyno4lOTepgwMx0vATGTnj8V5nzHW//9y1onW2vGpDXWBsR62WQ/brIdt1sO2C2g9DPs7GJ03gQAADEYBBAAYjALIXlhN8k92vo7MethmPWyzHrZZD9usB2bKm0AAAAZjDyAAwGAUQACAwSiAAACDUQABAAajAPKUqqqfqqq7qupMVX26qv76rGeapqp6a1W1cy5HZz3XNFTVK6rqA1V1787P/bpzbq+d9XNvVZ2uqjuq6vmzmncvPIl18N7zbB8fn9W8e6WqbqmqT1bViaq6v6reX1Xfec59lqvqnVX1YFWdrKrfrKrrZjXzXniS6+GO82wTvzyrmRmHAshTpqpen+QdSX4+yXcn+cMkt1XVM2Y62PR9Lsk1uy4vnO04U3MgyZ8k+enHuP0fJfnZndtfkuRokt+tqkPTGW8qnmgdJMm/yTdvH6+ZwlzT9sok707y0iSvzvanTt1eVQd23ecdSX44yY8keXmSg0l+q6rmpzzrXnoy6yFJ3pNv3iZ+cppDMiangeEpU1WfSPKZ1tqbdl33Z0ne31q7ZXaTTU9VvTXJ61prN896llmqqpbkh1tr79/5vpLcm+QdrbX/cee65ST3JfmvW2v/fGbD7pFz18HOde9Ncllr7XWPGdyHqurKJPcneWVr7Q+q6kiSB5L83dbar+zc59okdyd5TWvtd2Y37d45dz3sXHdHkj9urb15lrMxHnsAeUpU1VKSFye5/Zybbk/ysulPNFM37bwEeFdV/XJVPXvWA10AbkhydXZtH6211SS/n/G2j1ftvBz4+ap6T1VdNeuBpuDIzteHd76+OMlivnl7uDfJndnf28O56+GsN+y8FP65qnrbPtsrzgVqYdYDsG9ckWQ+23t0drsv2//wj+ITSX4syeeTfFuSf5zkY1X1/NbaQzOdbLbObgPn2z6eOeVZZum2JP86yZezXYr/uyS/V1Uv3inE+87O3t+3J/loa+3OnauvTrLWWnvknLvv278Xj7EekuRfJbkr24dEvCDJrUm+K9svGcOeUQB5qp17TEGd57p9q7V2265vP1tVf5Tki0l+PNt//Ec3+vbxK7u+vbOqPpXtMviDSX5tNlPtuXcleVG2j/N7Ivt5ezjvemitvWfXt3dW1ReSfKqqvqe19plpDshYvATMU+XBJJv51mfvV+Vb9/oMo7V2Mslnk9w061lm7Ow7oW0fu7TWvpbtArgvt4+qemeSH0ryH7bW7tl109EkS1X1tHMi+3J7eJz1cD6fSbKefbpNcOFQAHlKtNbWknw63/qyxauTfGz6E10Ydt7o8NwkX5v1LDN29iWub2wfO8eNvjJjbx9PT3J99tn2sXPKn3cl+TtJvq+1dtc5d/l0tkvO7u3hmmy/BLpvtocnsR7O5/nZPj5yX20TXHi8BMxT6e1J/sXOy1p/lOTvJ3lGkn8206mmqKreluQDSb6MF6+dAAAB40lEQVSS7b0Z/zjJ4STvm+Vc01BVB5M8Z9dVN1TVzUkebq19parekeQtOy9xfSHJW5KcSvJL0592bzzeOti5vDXJr2b7H/dnJfmn2d57/utTHXTvvTvJjyZ5bZITVXV2z++x1trp1tqxqvrFJL9QVQ9le928Ldt7yz80k4n3xuOuh6q6Mckbknww29vB85L8QpJ/l+TfzmBeBuI0MDylquqnsn2+t2uy/Y6+nzl7uoMR7JzA9RXZflPMA0k+nuS/aa396UwHm4KqelWSj5znpve11n5i5yD4n8v2Oc6elu03zPyDcw6Iv6g93jpI8qYk78/2OTIvy3YJ/Ei2t4+7pzXjNOycAud83thae+/OfVaS/M/ZLkiXJPlwkp/aT+viidZDVV2f5F9me8/nwWyfBue3k/yT1tq57xSGp5QCCAAwGMcAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg1EAAQAGowACAAxGAQQAGIwCCAAwGAUQAGAwCiAAwGAUQACAwSiAAACDUQABAAajAAIADEYBBAAYjAIIADAYBRAAYDAKIADAYBRAAIDBKIAAAINRAAEABqMAAgAMRgEEABiMAggAMBgFEABgMAogAMBgFEAAgMEogAAAg/n/AIK10Vs5oOddAAAAAElFTkSuQmCC"]}}],"execution_count":6},{"cell_type":"markdown","source":["The data set is loaded into memory and will be normalized so the pixel values in each image range between 0-1 insteaed of 0-255 pixel.   This will help the model converge faster because it is working with smaller numbers.\n\nIn this section we also one-hot encode the labels for each image.  This is done so the model does not treat the label (fashion category) with an implied numeric ranking.  For instance, T-Shirts are label '0' and Sneakers are label '7'. We don't want the model to interpret these values with any numeric ranking. An array will be created and each image will be represented in the array with a '1' corresponding to its label. For instance, an image with an array that looks like this: array([0,0,0,0,0,0,0,0,0,1]) means the image label corresponds to the 9th index which is 'ankle boots'."],"metadata":{}},{"cell_type":"code","source":["# Set number of categories\nnum_classes = 10\n\n# Set image size to 28*28 pixels\nimg_rows,img_cols = 28,28\n\n# Reshape the array without changing the data.  \n# Parameters = number of elements in the input array, the new shape (28*28), and the order \nx_train = x_train.reshape(60000, img_rows, img_cols, 1)\nx_test = x_test.reshape(10000, img_rows, img_cols, 1)\n\n#Type convert and scale the data\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n# One-hot encoding of images\n# This creates an array equal to the number of labels (fashion categories).  Each image\n# is represented in the vector with a 1 corresponding to it's label.  \n# For instance, an image with an array that looks like this: array([0,0,0,0,0,0,0,0,0,1]) means the image\n# participates in the 10th category which is 'ankle boots'\n\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Now we are able to define the Convolutional Neural Network (CNN) in layers\n\n![CNN](https://raw.githubusercontent.com/amynic/azureml-sdk-fashion/master/images/cnn.JPG \"CNN\")\n\n* This is a **sequential model** meaning every layer passes information forward to the next layer of the network\n* **1st Convoltuional Layer** - extracts features from data source, these are kernels/filters and feature maps. Feature maps passed to the  next layer. This layer also has a ReLu activation function - Y = max(0, x) this removes any value <0 and prevents vanishing gradients or weights <0\n* **2nd pooling layer ** - reduces dimensionality, reduce compute and helps with overfitting of the data.\n* **3rd Convolutional Layer ** -we add a Convoltuional Layer - extracts features from data source, these are kernels/filters and feature maps. Feature maps passed to the  next layer. This layer also has a ReLu activation function - Y = max(0, x) this removes any value <0 and prevents vanishing gradients or weights <0\n* **4th Pooling Layer ** - reduces dimensionality, reduce compute and helps with overfitting of the data.\n* **5th/6th Dense fully connected layer with softmax function:** put features together and classify what item of clothing is used"],"metadata":{}},{"cell_type":"markdown","source":["Use the Keras API to define the architecture.  Initially we will use the sequential API and compile the model with a common algorithm and loss metric\n\nAfter running through the experiment, come back to this section and change some of the parameters, re-run the code, and see how the accuracy and model will change.   Consider changing:\n  Add a drop out layer after the first pooling layer and also before the final dense layer:  model.add(Dropout(0.5))\n  Change the value of the dropout between 0 and 1:  model.add(Dropout(X))\n  Change the 2 Conv2D layer first variable to 32 instead of 64:  model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu'))\n  Add padding to each of the Conv2D layers: model.add(Conv2D(32, kernel_size=(3,3), padding = 'same', activation = 'relu'))"],"metadata":{}},{"cell_type":"code","source":["#First, set some parameters\n\n# Batch size is the sample size that will be processed independently, in parallel\nbatch_size =256\n\n# The number of training passes\nepochs = 10\n\ninput_shape = (img_rows, img_cols, 1)\n\n\n#Define the CNN model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 kernel_initializer='he_normal',\n                 input_shape=input_shape))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=losses.categorical_crossentropy,\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel.summary()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 11, 11, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 5, 5, 32)          0         \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 5, 5, 32)          0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 3, 3, 32)          9248      \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 3, 3, 32)          0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 288)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 32)                9248      \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 28,394\nTrainable params: 28,394\nNon-trainable params: 0\n_________________________________________________________________\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["Training the model is as simple as using the fit() method in Keras.   This code compiles the CNN model and assigns a loss/optimiser function and identifies the output metrics to view  \n\nA timer is started to show how long the model takes to run.\nThe model is trained using the epoch and batch_size parameters defined earlier. The test data set is passed in as the validation set so we can see how the accuracy differs between the training set and the validation set.\n\nThe opimizer is set to softmax but the value can be changed.  Here are some other options:  https://keras.io/optimizers/"],"metadata":{}},{"cell_type":"code","source":["# Train the model and return loss and accuracy for each epoch/training iteration\nstart = time.time()\nhist = model.fit(x_train, y_train, \n                  batch_size=batch_size, \n                  epochs=epochs, verbose=1)\nend = time.time()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\nError while obtaining a new communication channel"]}}],"execution_count":13},{"cell_type":"code","source":["# Print how long it took to train the  model\nprint('Time to train model (sec): ', (end-start))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Pass in the test data (x_test and y_test) to the evaluate method and Keras will score the model."],"metadata":{}},{"cell_type":"code","source":["#evaluate the model on the test data\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test Loss: ', score[0])\nprint('Test Accuracy: ', score[1])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Plot model accuracy across the epochs \n\naccuracy = hist.history['acc']\n#print(accuracy)\n#validation_accuracy = hist.history['val_acc']\n\nepochs = range(len(accuracy))\n\nfig = plt.figure()\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.title('Training accuracy')\nplt.legend()\n\ndisplay(fig)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Plot model loss across the epochs\n\nloss = hist.history['loss']\n#validation_loss = hist.history['val_loss']\n\nfig = plt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.title('Training loss')\nplt.legend()\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Run this code to see if the labels for each of the 15 images was predicted correctly\npredictions = model.predict(x_test)\n\n# Plot a random sample of 10 test images, their predicted labels and ground truth\nfigure = plt.figure(figsize=(20, 8))\nfor i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n    # Display each image\n    ax.imshow(np.squeeze(x_test[index]))\n    predict_index = np.argmax(predictions[index])\n    true_index = np.argmax(y_test[index])\n    # Set the title for each image\n    ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index], \n                                  fashion_mnist_labels[true_index]),\n                                  color=(\"green\" if predict_index == true_index else \"red\"))\n#Visualize in databricks\ndisplay()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["To easily see the impact of changing the activation function, a python function is defined below to construct, compile, train, and score a model.   The code below is taken from earlier cells."],"metadata":{}},{"cell_type":"code","source":["def runCNN(activation, verbose):\n\n  img_rows, img_cols = 28, 28\n  input_shape = (img_rows, img_cols, 1)\n  epochs = 1\n  \n  # Building up our CNN\n  model = Sequential()\n  \n  # Convolution Layer\n  model.add(Conv2D(32, kernel_size=(3, 3),\n                 activation=activation,\n                 kernel_initializer='he_normal',\n                 input_shape = input_shape)) \n  \n  # Pooling with stride (2, 2)\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  \n  # Randomly delete 25% of neurons to avoid overfitting  \n  model.add(Dropout(0.25))\n\n  model.add(Conv2D(32, (3, 3), activation=activation))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Dropout(0.25))\n  model.add(Conv2D(32, (3, 3), activation=activation))\n  model.add(Dropout(0.4))\n  \n  # Flatten layer \n  model.add(Flatten())\n  \n  # Fully connected Layer\n  model.add(Dense(32, activation=activation))\n  \n  # Randomely delete neurons to avoid overfitting \n  model.add(Dropout(0.3))\n  \n  # Apply Softmax\n  model.add(Dense(num_classes, activation='softmax'))\n\n  # Loss function (crossentropy) and Optimizer (Adam)\n  model.compile(loss = losses.categorical_crossentropy,\n              optimizer = optimizers.Adam(),\n              metrics=['accuracy'])\n \n  # Train model\n  model.fit(x_train, y_train, \n                batch_size=batch_size, \n                epochs=epochs, verbose=0)\n\n  # Evaluate our model\n  score = model.evaluate(x_test, y_test, verbose=0)\n  \n  # Return\n  return score"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Signmoid activation  \n\nscore_sigmoid = runCNN('sigmoid', 0)\nprint('Sigmoid, Test loss:', score_sigmoid[0])\nprint('Sigmoid, Test accuracy:', score_sigmoid[1])\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Tanh activation\n\nscore_tanh = runCNN('tanh', 0)\nprint('tanh, Test loss:', score_tanh[0])\nprint('tanh, Test accuracy:', score_tanh[1])\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Relu activation\n\nscore_sigmoid = runCNN('sigmoid', 0)\nprint('Sigmoid, Test loss:', score_sigmoid[0])\nprint('Sigmoid, Test accuracy:', score_sigmoid[1])\n"],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.2","nbconvert_exporter":"python","file_extension":".py"},"name":"FashionMNISTCNN","notebookId":3165046137871571,"kernelspec":{"display_name":"Python [myenv]","language":"python","name":"Python [myenv]"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
