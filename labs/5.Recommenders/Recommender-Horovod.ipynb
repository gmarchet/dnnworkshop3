{"cells":[{"cell_type":"markdown","source":["# Book Recommender"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\n\n\nfrom keras.layers import Input, Embedding, Flatten, Dot, Dense, Reshape, Concatenate, Dropout\nfrom keras.models import Model, load_model\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nfrom keras import regularizers\n\nfrom sklearn.model_selection import train_test_split\n\n#warnings.filterwarnings('ignore')\n%matplotlib inline"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\n%matplotlib inline is not supported in Databricks.\nYou can display matplotlib figures using display(). For an example, see https://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot.html\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["##Import book rating data"],"metadata":{}},{"cell_type":"code","source":["#use mount point you previously created when setting up the cluster\n# Open and read the file from mounted storage\ncontainer = \"data/5.Recommenders\"\ninputFilePath = \"/dbfs/mnt/{}/{}\".format(container, 'ratings.csv') \ndataset = pd.read_csv(inputFilePath)\ndataset.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: \n   book_id  user_id  rating\n0        1      314       5\n1        1      439       3\n2        1      588       5\n3        1     1169       4\n4        1     1185       4\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Let's have a look at how many data points we have"],"metadata":{}},{"cell_type":"code","source":["dataset.shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: (981756, 3)\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Let's check how many users and items we have"],"metadata":{}},{"cell_type":"code","source":["n_users = len(dataset.user_id.unique())\nprint(\"Number of users: \", n_users)\nn_items = len(dataset.book_id.unique())\nprint(\"Number of items: \", n_items)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of users:  53424\nNumber of items:  10000\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["To visualize the model, you can use Netron https://lutzroeder.github.io/netron/ in a browser.\nDownload the h5 file locally and point Netron to it."],"metadata":{}},{"cell_type":"markdown","source":["Read book data"],"metadata":{}},{"cell_type":"code","source":["inputFilePath = \"/dbfs/mnt/{}/{}\".format(container, 'books.csv') \nbooks = pd.read_csv(inputFilePath)\nbooks.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: \n  ...\n0 ...\n1 ...\n2 ...\n3 ...\n4 ...\n\n[5 rows x 23 columns]\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["## Add item metadata\nWe extend the model with data pertaining to the items. This requires numerical embedding of titles"],"metadata":{}},{"cell_type":"code","source":["books_subset=books[['id', 'original_title']]\nbooks_subset=books_subset.rename({'id':'book_id'}, axis='columns')\n\nbooks_subset.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: \n  ...\n0 ...\n1 ...\n2 ...\n3 ...\n4 ...\n\n[5 rows x 2 columns]\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#create a new dataset by joining the title information\nnew_dataset=dataset.join(books_subset.set_index('book_id'), on='book_id')\nnew_dataset.head()\nn_metadata=len(new_dataset.original_title.unique())\nn_metadata"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: 9275\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Embedding only works on integer vectors. We need to encode the titles."],"metadata":{}},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n\ntitles=np.array(new_dataset.original_title)\n\nt=Tokenizer()\ntitles=[str(i) for i in new_dataset.original_title]\nt.fit_on_texts(titles)\n\nvocab_size = len(t.word_index) + 1\n# integer encode the documents\nencoded_titles = t.texts_to_sequences(titles)\n\nfrom keras.preprocessing.sequence import pad_sequences\nmax_length = 10\npadded_titles = pad_sequences(encoded_titles, \n                              maxlen=max_length, \n                              padding='post')\npadded_titles"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: \narray([[  1, 965, 525, ...,   0,   0,   0],\n       [  1, 965, 525, ...,   0,   0,   0],\n       [  1, 965, 525, ...,   0,   0,   0],\n       ...,\n       [  1,  63,  20, ...,   0,   0,   0],\n       [  1,  63,  20, ...,   0,   0,   0],\n       [  1,  63,  20, ...,   0,   0,   0]], dtype=int32)\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Join the encoded titles to the dataset\nnew_dataset=new_dataset.join(pd.DataFrame(padded_titles))\nnew_dataset.head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: \n   book_id  user_id  rating    original_title  0    1 ...  4  5  6  7  8  9\n0        1      314       5  The Hunger Games  1  965 ...  0  0  0  0  0  0\n1        1      439       3  The Hunger Games  1  965 ...  0  0  0  0  0  0\n2        1      588       5  The Hunger Games  1  965 ...  0  0  0  0  0  0\n3        1     1169       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n4        1     1185       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n5        1     2077       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n6        1     2487       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n7        1     2900       5  The Hunger Games  1  965 ...  0  0  0  0  0  0\n8        1     3662       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n9        1     3922       5  The Hunger Games  1  965 ...  0  0  0  0  0  0\n\n[10 rows x 14 columns]\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["#split for training\ntrain, test = train_test_split(new_dataset, test_size=0.1, random_state=42)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["##Distributed training with Horovod"],"metadata":{}},{"cell_type":"code","source":["import time\nFUSE_MOUNT_LOCATION = '/dbfs/horovod_keras/'\ncheckpoint_dir = FUSE_MOUNT_LOCATION + '{}/'.format(time.time())\nos.makedirs(checkpoint_dir)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["###Build a new hybrid recommender model\nUse embedded item metadata in addition to user, item and rating information"],"metadata":{}},{"cell_type":"code","source":["def shard_data (train_data, rank=0, size=1):\n  train_data=train_data[rank::size]\n  return (train_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["def get_model(n_items, n_users, embedding_size=5):\n  item_input = Input(shape=[1], name=\"Item-Input\")\n  item_embedding = Embedding(input_dim=n_items+1, output_dim=embedding_size, name=\"Item-Embedding\")(item_input)\n  item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n\n  user_input = Input(shape=[1], name=\"User-Input\")\n  user_embedding = Embedding(input_dim=n_users+1, output_dim=embedding_size, name=\"User-Embedding\")(user_input)\n  user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n\n  metadata_input=Input(shape=[max_length], name=\"Metadata-Input\")\n  metadata_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size, name=\"Metadata-Embedding\")(metadata_input)\n  metadata_vec = Flatten(name=\"Flatten-Metadata\")(metadata_embedding)\n\n  # new model with metadata and several regularization techniques to prevent overfitting\n  input_vecs = Concatenate()([user_vec, item_vec, metadata_vec])\n  input_vecs = Dropout(0.5)(input_vecs)\n  x = Dense(128, activation='relu',\n            #kernel_regularizer=regularizers.l2(0.01),\n            kernel_initializer='random_normal',\n            activity_regularizer=regularizers.l1(10e-4) #to prevent overfitting of training data\n           )(input_vecs)\n  #x = Dropout(0.5)(x)\n  #output layer\n  y = Dense(1)(x)\n\n  new_model = Model([user_input, item_input, metadata_input], y)\n  return (new_model)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["## train new model with sampling\nnb_epoch = 12\nbatch_size = 32\noutputFilePath = \"/dbfs/mnt/{}/{}\".format(container, 'hybrid_deep_model.h5')"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["import keras\nimport horovod.keras as hvd\n\nfrom keras import backend as K\nimport tensorflow as tf\n\ndef train_model(train, n_items, n_users):\n  # Horovod: initialize Horovod.\n  hvd.init()\n  \n  # Horovod: pin GPU to be used to process local rank (one GPU per process)\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  config.gpu_options.visible_device_list = str(hvd.local_rank())\n  K.set_session(tf.Session(config=config))\n  \n  #get data\n  train_data=shard_data(train)\n  #get model\n  new_model=get_model(n_items, n_users, 5)\n  \n  # Horovod: adjust learning rate based on number of GPUs.\n  learning_rate=1.0\n  optimizer = keras.optimizers.Adadelta(learning_rate * hvd.size())\n\n  # Horovod: Wrap optimizer with Horovod DistributedOptimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n  \n  #compile model\n  new_model.compile(optimizer=optimizer, loss='mean_squared_error')\n  \n  # Horovod: Broadcast initial variable states from rank 0\n  # to all other processes. This is necessary to ensure \n  # consistent initialization of all workers when training is\n  # started with random weights or restored from a checkpoint.\n  callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0)]\n  \n  # Horovod: Save checkpoints only on worker 0 to prevent \n  # other workers from overwriting and corrupting them.\n  if hvd.rank() == 0:\n    callbacks.append(keras.callbacks.ModelCheckpoint(filepath=outputFilePath, save_best_only=True))\n\n  #more callbacks\n  earlystop = EarlyStopping(monitor='val_loss',\n                           patience=5,\n                           verbose=1,\n                           restore_best_weights=True\n                           )\n  callbacks.append(earlystop)\n  # train model\n  new_model.fit([train_data.user_id, train_data.book_id, train_data.iloc[:,4:4+max_length]], train_data.rating,\n                      epochs=nb_epoch,\n                      batch_size=batch_size,\n                      shuffle=True,\n                      validation_split=0.1,\n                      verbose=2,\n                      callbacks=callbacks)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["from sparkdl import HorovodRunner\n\nhr = HorovodRunner(np=2)\nhr.run(train_model, train=train, n_items=n_items, n_users=n_users)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The global names read or written to by the pickled function are {&#39;EarlyStopping&#39;, &#39;max_length&#39;, &#39;str&#39;, &#39;outputFilePath&#39;, &#39;keras&#39;, &#39;batch_size&#39;, &#39;hvd&#39;, &#39;shard_data&#39;, &#39;tf&#39;, &#39;K&#39;, &#39;get_model&#39;, &#39;nb_epoch&#39;}.\nThe pickled object size is 68410683 bytes.\nThe pickled object size is greater than 10MB. It might cause training slow to start. You might\nconsider:\n* Loading large datasets inside the main function instead of materializing them on the driver.\n* Avoid pulling unnecessary variables from the notebook context.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n\n### Where to find training logs? ###\nYou can find the stdout and stderr of this training session in Spark UI. Expand the job progress bar\nuntil you see the (i) icon next to the single stage from this job. Click it and then click &#34;stderr&#34;\ncorresponding to Task 0. You can use cluster log delivery feature to archive the logs.\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["new_model= load_model(outputFilePath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["eval_loss=new_model.evaluate([test.user_id, test.book_id, test.iloc[:, 4:4+max_length]], test.rating)\nprint(\"Evaluation loss: \", eval_loss)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n   32/98176 [..............................] - ETA: 3:10\n 1792/98176 [..............................] - ETA: 6s  \n 3584/98176 [&gt;.............................] - ETA: 4s\n 5440/98176 [&gt;.............................] - ETA: 3s\n 7296/98176 [=&gt;............................] - ETA: 3s\n 9152/98176 [=&gt;............................] - ETA: 3s\n11008/98176 [==&gt;...........................] - ETA: 2s\n12864/98176 [==&gt;...........................] - ETA: 2s\n14720/98176 [===&gt;..........................] - ETA: 2s\n16576/98176 [====&gt;.........................] - ETA: 2s\n18368/98176 [====&gt;.........................] - ETA: 2s\n20128/98176 [=====&gt;........................] - ETA: 2s\n21920/98176 [=====&gt;........................] - ETA: 2s\n23328/98176 [======&gt;.......................] - ETA: 2s\n25152/98176 [======&gt;.......................] - ETA: 2s\n26944/98176 [=======&gt;......................] - ETA: 2s\n28800/98176 [=======&gt;......................] - ETA: 2s\n30624/98176 [========&gt;.....................] - ETA: 2s\n32480/98176 [========&gt;.....................] - ETA: 1s\n34272/98176 [=========&gt;....................] - ETA: 1s\n36096/98176 [==========&gt;...................] - ETA: 1s\n37920/98176 [==========&gt;...................] - ETA: 1s\n39712/98176 [===========&gt;..................] - ETA: 1s\n41568/98176 [===========&gt;..................] - ETA: 1s\n43456/98176 [============&gt;.................] - ETA: 1s\n45280/98176 [============&gt;.................] - ETA: 1s\n46976/98176 [=============&gt;................] - ETA: 1s\n48800/98176 [=============&gt;................] - ETA: 1s\n50624/98176 [==============&gt;...............] - ETA: 1s\n52480/98176 [===============&gt;..............] - ETA: 1s\n54304/98176 [===============&gt;..............] - ETA: 1s\n56096/98176 [================&gt;.............] - ETA: 1s\n57856/98176 [================&gt;.............] - ETA: 1s\n59008/98176 [=================&gt;............] - ETA: 1s\n60832/98176 [=================&gt;............] - ETA: 1s\n62432/98176 [==================&gt;...........] - ETA: 1s\n64256/98176 [==================&gt;...........] - ETA: 0s\n66080/98176 [===================&gt;..........] - ETA: 0s\n67872/98176 [===================&gt;..........] - ETA: 0s\n69632/98176 [====================&gt;.........] - ETA: 0s\n71360/98176 [====================&gt;.........] - ETA: 0s\n72992/98176 [=====================&gt;........] - ETA: 0s\n74720/98176 [=====================&gt;........] - ETA: 0s\n76480/98176 [======================&gt;.......] - ETA: 0s\n78240/98176 [======================&gt;.......] - ETA: 0s\n80032/98176 [=======================&gt;......] - ETA: 0s\n81824/98176 [========================&gt;.....] - ETA: 0s\n83552/98176 [========================&gt;.....] - ETA: 0s\n85312/98176 [=========================&gt;....] - ETA: 0s\n87104/98176 [=========================&gt;....] - ETA: 0s\n88864/98176 [==========================&gt;...] - ETA: 0s\n90624/98176 [==========================&gt;...] - ETA: 0s\n92352/98176 [===========================&gt;..] - ETA: 0s\n94112/98176 [===========================&gt;..] - ETA: 0s\n95904/98176 [============================&gt;.] - ETA: 0s\n97696/98176 [============================&gt;.] - ETA: 0s\n98176/98176 [==============================] - 3s 29us/step\nEvaluation loss:  0.7305876850265879\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["Less overfit, better mse overall!"],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.6","nbconvert_exporter":"python","file_extension":".py"},"name":"Recommender-Horovod","notebookId":2035694585769592},"nbformat":4,"nbformat_minor":0}
