{"cells":[{"cell_type":"markdown","source":["# Book Recommender"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\n\n\nfrom keras.layers import Input, Embedding, Flatten, Dot, Dense, Reshape, Concatenate, Dropout\nfrom keras.models import Model, load_model\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nfrom keras import regularizers\n\nfrom sklearn.model_selection import train_test_split\n\n#warnings.filterwarnings('ignore')\n%matplotlib inline\n# TIMESTAMP IMPORT MUST BE CHANGED WHEN USING PANDAS 23.0+\n#from pandas.lib import Timestamp\n# CHANGE TO THE FOLLOWING\nfrom pandas._libs.tslibs.timestamps import Timestamp"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">%matplotlib inline is not supported in Databricks.\nYou can display matplotlib figures using display(). For an example, see https://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot.html\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["##Import book rating data"],"metadata":{}},{"cell_type":"code","source":["#use mount point you previously created when setting up the cluster\n# Open and read the file from mounted storage\ncontainer = \"data/5.Recommenders\"\ninputFilePath = \"/dbfs/mnt/{}/{}\".format(container, 'ratings.csv') \ndataset = pd.read_csv(inputFilePath)\ndataset.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: \n   book_id  user_id  rating\n0        1      314       5\n1        1      439       3\n2        1      588       5\n3        1     1169       4\n4        1     1185       4\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Let's have a look at how many data points we have"],"metadata":{}},{"cell_type":"code","source":["dataset.shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: (981756, 3)\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Let's check how many users and items we have"],"metadata":{}},{"cell_type":"code","source":["n_users = len(dataset.user_id.unique())\nprint(\"Number of users: \", n_users)\nn_items = len(dataset.book_id.unique())\nprint(\"Number of items: \", n_items)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of users:  53424\nNumber of items:  10000\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["To visualize the model, you can use Netron https://lutzroeder.github.io/netron/ in a browser.\nDownload the h5 file locally and point Netron to it."],"metadata":{}},{"cell_type":"markdown","source":["Read book data"],"metadata":{}},{"cell_type":"code","source":["inputFilePath = \"/dbfs/mnt/{}/{}\".format(container, 'books.csv') \nbooks = pd.read_csv(inputFilePath)\nbooks.head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: \n  ...\n0 ...\n1 ...\n2 ...\n3 ...\n4 ...\n5 ...\n6 ...\n7 ...\n8 ...\n9 ...\n\n[10 rows x 23 columns]\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["books.loc[0]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[47]: \nid                                                                           1\nbook_id                                                                2767052\nbest_book_id                                                           2767052\nwork_id                                                                2792775\nbooks_count                                                                272\nisbn                                                                 439023483\nisbn13                                                             9.78044e+12\nauthors                                                        Suzanne Collins\noriginal_publication_year                                                 2008\noriginal_title                                                The Hunger Games\ntitle                                  The Hunger Games (The Hunger Games, #1)\nlanguage_code                                                              eng\naverage_rating                                                            4.34\nratings_count                                                          4780653\nwork_ratings_count                                                     4942365\nwork_text_reviews_count                                                 155254\nratings_1                                                                66715\nratings_2                                                               127936\nratings_3                                                               560092\nratings_4                                                              1481305\nratings_5                                                              2706317\nimage_url                    https://images.gr-assets.com/books/1447303603m...\nsmall_image_url              https://images.gr-assets.com/books/1447303603s...\nName: 0, dtype: object\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["## Add item metadata\nWe extend the model with data pertaining to the items. This requires numerical embedding of titles"],"metadata":{}},{"cell_type":"code","source":["books_subset=books[['id', 'original_title']]\nbooks_subset=books_subset.rename({'id':'book_id'}, axis='columns')\n\nbooks_subset.head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[34]: \n  ...\n0 ...\n1 ...\n2 ...\n3 ...\n4 ...\n5 ...\n6 ...\n7 ...\n8 ...\n9 ...\n\n[10 rows x 2 columns]\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["#create a new dataset by joining the title information\nnew_dataset=dataset.join(books_subset.set_index('book_id'), on='book_id')\nnew_dataset.head()\nn_metadata=len(new_dataset.original_title.unique())\nn_metadata"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: 9275\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["Embedding only works on integer vectors. We need to encode the titles."],"metadata":{}},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n\ntitles=np.array(new_dataset.original_title)\n\nt=Tokenizer()\ntitles=[str(i) for i in new_dataset.original_title]\nt.fit_on_texts(titles)\n\nvocab_size = len(t.word_index) + 1\n# integer encode the documents\nencoded_titles = t.texts_to_sequences(titles)\n\nfrom keras.preprocessing.sequence import pad_sequences\nmax_length = 10\npadded_titles = pad_sequences(encoded_titles, \n                              maxlen=max_length, \n                              padding='post')\npadded_titles"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: \narray([[  1, 965, 525, ...,   0,   0,   0],\n       [  1, 965, 525, ...,   0,   0,   0],\n       [  1, 965, 525, ...,   0,   0,   0],\n       ...,\n       [  1,  63,  20, ...,   0,   0,   0],\n       [  1,  63,  20, ...,   0,   0,   0],\n       [  1,  63,  20, ...,   0,   0,   0]], dtype=int32)\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# Join the encoded titles to the dataset\nnew_dataset=new_dataset.join(pd.DataFrame(padded_titles))\nnew_dataset.head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: \n   book_id  user_id  rating    original_title  0    1 ...  4  5  6  7  8  9\n0        1      314       5  The Hunger Games  1  965 ...  0  0  0  0  0  0\n1        1      439       3  The Hunger Games  1  965 ...  0  0  0  0  0  0\n2        1      588       5  The Hunger Games  1  965 ...  0  0  0  0  0  0\n3        1     1169       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n4        1     1185       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n5        1     2077       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n6        1     2487       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n7        1     2900       5  The Hunger Games  1  965 ...  0  0  0  0  0  0\n8        1     3662       4  The Hunger Games  1  965 ...  0  0  0  0  0  0\n9        1     3922       5  The Hunger Games  1  965 ...  0  0  0  0  0  0\n\n[10 rows x 14 columns]\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["#split for training\ntrain, test = train_test_split(new_dataset, test_size=0.1, random_state=42)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["##Distributed training with Horovod"],"metadata":{}},{"cell_type":"code","source":["import time\nFUSE_MOUNT_LOCATION = '/dbfs/horovod_keras/'\ncheckpoint_dir = FUSE_MOUNT_LOCATION + '{}/'.format(time.time())\nos.makedirs(checkpoint_dir)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["###Build a new hybrid recommender model\nUse embedded item metadata in addition to user, item and rating information"],"metadata":{}},{"cell_type":"code","source":["def shard_data (train_data, rank=0, size=1):\n  train_data=train_data[rank::size]\n  return (train_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["def get_model(n_items, n_users, embedding_size=5):\n  item_input = Input(shape=[1], name=\"Item-Input\")\n  item_embedding = Embedding(input_dim=n_items+1, output_dim=embedding_size, name=\"Item-Embedding\")(item_input)\n  item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n\n  user_input = Input(shape=[1], name=\"User-Input\")\n  user_embedding = Embedding(input_dim=n_users+1, output_dim=embedding_size, name=\"User-Embedding\")(user_input)\n  user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n\n  metadata_input=Input(shape=[max_length], name=\"Metadata-Input\")\n  metadata_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size, name=\"Metadata-Embedding\")(metadata_input)\n  metadata_vec = Flatten(name=\"Flatten-Metadata\")(metadata_embedding)\n\n  # new model with metadata and several regularization techniques to prevent overfitting\n  input_vecs = Concatenate()([user_vec, item_vec, metadata_vec])\n  input_vecs = Dropout(0.5)(input_vecs)\n  x = Dense(128, activation='relu',\n            #kernel_regularizer=regularizers.l2(0.01),\n            kernel_initializer='random_normal',\n            activity_regularizer=regularizers.l1(10e-4) #to prevent overfitting of training data\n           )(input_vecs)\n  #x = Dropout(0.5)(x)\n  #output layer\n  y = Dense(1)(x)\n\n  new_model = Model([user_input, item_input, metadata_input], y)\n  return (new_model)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["## train new model with sampling\nnb_epoch = 12\nbatch_size = 128 # batch is bigger because execution is split across GPUs\noutputFilePath = \"/dbfs/mnt/{}/{}\".format(container, 'hybrid_deep_model.h5')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["import keras\nimport horovod.keras as hvd\n\nfrom keras import backend as K\nimport tensorflow as tf\n\ndef train_model(train, n_items, n_users):\n  # Horovod: initialize Horovod.\n  hvd.init()\n  \n  # Horovod: pin GPU to be used to process local rank (one GPU per process)\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  config.gpu_options.visible_device_list = str(hvd.local_rank())\n  K.set_session(tf.Session(config=config))\n  \n  #get data\n  train_data=shard_data(train, hvd.rank(), hvd.size())\n  #get model\n  new_model=get_model(n_items, n_users, 5)\n  \n  # Horovod: adjust learning rate based on number of GPUs.\n  learning_rate=1.0\n  optimizer = keras.optimizers.Adadelta(learning_rate * hvd.size())\n\n  # Horovod: Wrap optimizer with Horovod DistributedOptimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n  \n  #compile model\n  new_model.compile(optimizer=optimizer, loss='mean_squared_error')\n  \n  # Horovod: Broadcast initial variable states from rank 0\n  # to all other processes. This is necessary to ensure \n  # consistent initialization of all workers when training is\n  # started with random weights or restored from a checkpoint.\n  callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0)]\n  \n  # Horovod: Save checkpoints only on worker 0 to prevent \n  # other workers from overwriting and corrupting them.\n  if hvd.rank() == 0:\n    callbacks.append(keras.callbacks.ModelCheckpoint(filepath=outputFilePath, save_best_only=True))\n\n  #more callbacks\n  earlystop = EarlyStopping(monitor='val_loss',\n                           patience=5,\n                           verbose=1,\n                           restore_best_weights=True\n                           )\n  callbacks.append(earlystop)\n  # train model\n  new_model.fit([train_data.user_id, train_data.book_id, train_data.iloc[:,4:4+max_length]], train_data.rating,\n                      epochs=nb_epoch,\n                      batch_size=batch_size,\n                      shuffle=True,\n                      validation_split=0.1,\n                      verbose=2,\n                      callbacks=callbacks)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["from sparkdl import HorovodRunner\n\nhr = HorovodRunner(np=2)\nhr.run(train_model, train=train, n_items=n_items, n_users=n_users)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The global names read or written to by the pickled function are {&#39;EarlyStopping&#39;, &#39;max_length&#39;, &#39;str&#39;, &#39;outputFilePath&#39;, &#39;keras&#39;, &#39;batch_size&#39;, &#39;hvd&#39;, &#39;shard_data&#39;, &#39;tf&#39;, &#39;K&#39;, &#39;get_model&#39;, &#39;nb_epoch&#39;}.\nThe pickled object size is 68410695 bytes.\nThe pickled object size is greater than 10MB. It might cause training slow to start. You might\nconsider:\n* Loading large datasets inside the main function instead of materializing them on the driver.\n* Avoid pulling unnecessary variables from the notebook context.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n\n### Where to find training logs? ###\nYou can find the stdout and stderr of this training session in Spark UI. Expand the job progress bar\nuntil you see the (i) icon next to the single stage from this job. Click it and then click &#34;stderr&#34;\ncorresponding to Task 0. You can use cluster log delivery feature to archive the logs.\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["new_model= load_model(outputFilePath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["eval_loss=new_model.evaluate([test.user_id, test.book_id, test.iloc[:, 4:4+max_length]], test.rating)\nprint(\"Evaluation loss: \", eval_loss)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n   32/98176 [..............................] - ETA: 4:09\n 1120/98176 [..............................] - ETA: 11s \n 2304/98176 [..............................] - ETA: 7s \n 3776/98176 [&gt;.............................] - ETA: 5s\n 4768/98176 [&gt;.............................] - ETA: 5s\n 6272/98176 [&gt;.............................] - ETA: 4s\n 7520/98176 [=&gt;............................] - ETA: 4s\n 8928/98176 [=&gt;............................] - ETA: 4s\n10304/98176 [==&gt;...........................] - ETA: 4s\n11744/98176 [==&gt;...........................] - ETA: 3s\n12736/98176 [==&gt;...........................] - ETA: 3s\n13920/98176 [===&gt;..........................] - ETA: 3s\n15552/98176 [===&gt;..........................] - ETA: 3s\n16448/98176 [====&gt;.........................] - ETA: 3s\n16896/98176 [====&gt;.........................] - ETA: 3s\n17696/98176 [====&gt;.........................] - ETA: 3s\n18496/98176 [====&gt;.........................] - ETA: 3s\n19936/98176 [=====&gt;........................] - ETA: 3s\n21568/98176 [=====&gt;........................] - ETA: 3s\n23360/98176 [======&gt;.......................] - ETA: 3s\n25216/98176 [======&gt;.......................] - ETA: 3s\n27040/98176 [=======&gt;......................] - ETA: 3s\n28864/98176 [=======&gt;......................] - ETA: 2s\n30688/98176 [========&gt;.....................] - ETA: 2s\n32544/98176 [========&gt;.....................] - ETA: 2s\n34336/98176 [=========&gt;....................] - ETA: 2s\n36160/98176 [==========&gt;...................] - ETA: 2s\n38016/98176 [==========&gt;...................] - ETA: 2s\n39776/98176 [===========&gt;..................] - ETA: 2s\n41440/98176 [===========&gt;..................] - ETA: 2s\n43264/98176 [============&gt;.................] - ETA: 2s\n45120/98176 [============&gt;.................] - ETA: 1s\n46976/98176 [=============&gt;................] - ETA: 1s\n48768/98176 [=============&gt;................] - ETA: 1s\n50592/98176 [==============&gt;...............] - ETA: 1s\n51904/98176 [==============&gt;...............] - ETA: 1s\n52992/98176 [===============&gt;..............] - ETA: 1s\n54560/98176 [===============&gt;..............] - ETA: 1s\n56320/98176 [================&gt;.............] - ETA: 1s\n58176/98176 [================&gt;.............] - ETA: 1s\n60032/98176 [=================&gt;............] - ETA: 1s\n61888/98176 [=================&gt;............] - ETA: 1s\n63520/98176 [==================&gt;...........] - ETA: 1s\n65344/98176 [==================&gt;...........] - ETA: 1s\n67232/98176 [===================&gt;..........] - ETA: 1s\n68992/98176 [====================&gt;.........] - ETA: 0s\n70752/98176 [====================&gt;.........] - ETA: 0s\n72448/98176 [=====================&gt;........] - ETA: 0s\n74240/98176 [=====================&gt;........] - ETA: 0s\n76032/98176 [======================&gt;.......] - ETA: 0s\n77888/98176 [======================&gt;.......] - ETA: 0s\n79744/98176 [=======================&gt;......] - ETA: 0s\n81568/98176 [=======================&gt;......] - ETA: 0s\n83392/98176 [========================&gt;.....] - ETA: 0s\n85248/98176 [=========================&gt;....] - ETA: 0s\n87008/98176 [=========================&gt;....] - ETA: 0s\n88864/98176 [==========================&gt;...] - ETA: 0s\n90560/98176 [==========================&gt;...] - ETA: 0s\n92256/98176 [===========================&gt;..] - ETA: 0s\n94048/98176 [===========================&gt;..] - ETA: 0s\n95872/98176 [============================&gt;.] - ETA: 0s\n97600/98176 [============================&gt;.] - ETA: 0s\n98176/98176 [==============================] - 3s 32us/step\nEvaluation loss:  0.7187896667842448\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["Less overfit, better mse overall!"],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.6","nbconvert_exporter":"python","file_extension":".py"},"name":"Recommender-Horovod","notebookId":2035694585769592},"nbformat":4,"nbformat_minor":0}
